# QLoRA å¿«é€Ÿå‚è€ƒå¡ç‰‡

## ğŸš€ æˆåŠŸåŠ è½½ LLaVA + QLoRA çš„æœ€å°ä»£ç 

```python
import torch
from transformers import BitsAndBytesConfig, LlavaForConditionalGeneration, AutoTokenizer

# é…ç½®
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

# åŠ è½½
model = LlavaForConditionalGeneration.from_pretrained(
    "liuhaotian/llava-v1.5-7b",
    quantization_config=quant_config,
    device_map="auto",
    torch_dtype=torch.float16,  # âš ï¸ å¿…é¡»ï¼ä¸åŠ ä¼šæ®µé”™è¯¯
)
```

## âš¡ å…³é”®è¦ç‚¹

1. **æ°¸è¿œæ·»åŠ  `torch_dtype=torch.float16`** - è¿™æ˜¯é¿å…æ®µé”™è¯¯çš„å…³é”®
2. **é‡åŒ–æ¨¡å¼ä¸è¦ç”¨ `trust_remote_code=True`**
3. **è®¾ç½®é•œåƒæº**: `export HF_ENDPOINT=https://hf-mirror.com`
4. **LLaVA processor è­¦å‘Šæ˜¯æ­£å¸¸çš„** - ä¼šè‡ªåŠ¨ fallback åˆ° tokenizer

## ğŸ”§ å¸¸è§é—®é¢˜é€ŸæŸ¥

| é”™è¯¯ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| æ®µé”™è¯¯ (139) | ç¼ºå°‘ torch_dtype | æ·»åŠ  `torch_dtype=torch.float16` |
| normal_kernel_cpu | åŒä¸Š | åŒä¸Š |
| preprocessor_config.json | LLaVA æ²¡è¿™ä¸ªæ–‡ä»¶ | æ­£å¸¸ï¼Œå¿½ç•¥è­¦å‘Š |
| Connection reset | ç½‘ç»œé—®é¢˜ | `export HF_ENDPOINT=https://hf-mirror.com` |

## ğŸ“Š æ˜¾å­˜ä½¿ç”¨å‚è€ƒ (RTX 3060 12GB)

```
LLaVA-7B + QLoRA:
  è®­ç»ƒä¸­: 4.32 GB
  å‰©ä½™: ~7 GB
  
å»ºè®®é…ç½®:
  batch_size: 1
  gradient_accumulation: 32
  max_length: 512
```

## ğŸ§ª æµ‹è¯•å‘½ä»¤

```bash
# è®¾ç½®é•œåƒæº + è¿è¡Œæµ‹è¯•
export HF_ENDPOINT=https://hf-mirror.com && \
cd /home/bygpu/Desktop/sam2/demo/training && \
python test_qlora_training.py --quick
```

## ğŸ“ ç›¸å…³æ–‡ä»¶

- å®Œæ•´æ–‡æ¡£: `docs/TROUBLESHOOTING.md`
- æˆåŠŸç¤ºä¾‹: `test_llava_4bit_final.py`
- ä¿®å¤ä½ç½®: `core/trainers/lora_trainer.py:113`
