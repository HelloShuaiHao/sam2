services:
  frontend:
    image: sam2/frontend
    build:
      context: ./demo/frontend
      dockerfile: frontend.Dockerfile
    ports:
      - 7262:80
    environment:
      # Training API endpoint (通过外部Nginx代理访问)
      # 前端通过 ai.bygpu.com:55305/sam2/ 访问
      # Training API 通过 ai.bygpu.com:55305/api/training/ 访问
      - VITE_API_URL=/api/training
    depends_on:
      - backend

  backend:
    image: sam2/backend
    build:
      context: .
      dockerfile: backend.Dockerfile
    ports:
      - 7263:5000
    volumes:
      - ./demo/data/:/data/:rw
    environment:
      - SERVER_ENVIRONMENT=DEV
      - GUNICORN_WORKERS=1
      # Inference API needs to have at least 2 threads to handle an incoming
      # parallel cancel propagation request
      - GUNICORN_THREADS=2
      - GUNICORN_PORT=5000
      - API_URL=http://ai.bygpu.com:55305/api/sam2
      - DEFAULT_VIDEO_PATH=gallery/test.mp4
      # # ffmpeg/video encode settings
      - FFMPEG_NUM_THREADS=1
      - VIDEO_ENCODE_CODEC=libx264
      - VIDEO_ENCODE_CRF=23
      - VIDEO_ENCODE_FPS=24
      - VIDEO_ENCODE_MAX_WIDTH=1280
      - VIDEO_ENCODE_MAX_HEIGHT=720
      - VIDEO_ENCODE_VERBOSE=False
      # PyTorch memory optimization
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:512
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  training-api:
    image: sam2/training-api
    build:
      context: .
      dockerfile: training-api.Dockerfile
    ports:
      - 7264:8000
    volumes:
      # Mount data directory for reading SAM2 exports
      - ./demo/data/:/data/:ro
      # Mount training output directory (writable)
      - ./demo/training/output/:/app/output/:rw
      # Mount checkpoints directory (writable)
      - ./checkpoints/:/app/checkpoints/:rw
      # Mount cache directory for HuggingFace models (persistent)
      - ./cache/huggingface/:/app/cache/huggingface/:rw
      - ./cache/transformers/:/app/cache/transformers/:rw
    environment:
      - SERVER_ENVIRONMENT=DEV
      # PyTorch settings
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:512
      - CUDA_VISIBLE_DEVICES=0
      # Enable detailed error logging
      - PYTHONFAULTHANDLER=1
      - CUDA_LAUNCH_BLOCKING=1
      # HuggingFace settings
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/transformers
      - HF_ENDPOINT=https://hf-mirror.com  # 使用国内镜像加速下载
      - HF_HUB_ENABLE_HF_TRANSFER=0  # 禁用 hf_transfer (与镜像站不兼容)
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      # 增加下载超时时间
      - REQUESTS_TIMEOUT=300
      - HTTP_TIMEOUT=300
      # Training API settings
      - MAX_WORKERS=1
      - LOG_LEVEL=info
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          # Increased memory limit for LLaVA model loading
          # QLoRA needs ~20GB for model + ~10GB for loading process
          memory: 48G
